// DO NOT EDIT.
//
// Generated by the Swift generator plugin for the protocol buffer compiler.
// Source: google/cloud/speech/v1/cloud_speech.proto
//
// For information on using the generated types, please see the documentation:
//   https://github.com/apple/swift-protobuf/

// Copyright 2019 Google LLC.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import Foundation
import SwiftProtobuf

// If the compiler emits an error on this type, it is because this file
// was generated by a version of the `protoc` Swift plug-in that is
// incompatible with the version of SwiftProtobuf to which you are linking.
// Please ensure that you are building against the same version of the API
// that was used to generate this file.
fileprivate struct _GeneratedWithProtocGenSwiftVersion: SwiftProtobuf.ProtobufAPIVersionCheck {
  struct _2: SwiftProtobuf.ProtobufAPIVersion_2 {}
  typealias Version = _2
}

/// The top-level message sent by the client for the `Recognize` method.
public struct Google_Cloud_Speech_V1_RecognizeRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. Provides information to the recognizer that specifies how to
  /// process the request.
  public var config: Google_Cloud_Speech_V1_RecognitionConfig {
    get {return _config ?? Google_Cloud_Speech_V1_RecognitionConfig()}
    set {_config = newValue}
  }
  /// Returns true if `config` has been explicitly set.
  public var hasConfig: Bool {return self._config != nil}
  /// Clears the value of `config`. Subsequent reads from it will return its default value.
  public mutating func clearConfig() {self._config = nil}

  /// Required. The audio data to be recognized.
  public var audio: Google_Cloud_Speech_V1_RecognitionAudio {
    get {return _audio ?? Google_Cloud_Speech_V1_RecognitionAudio()}
    set {_audio = newValue}
  }
  /// Returns true if `audio` has been explicitly set.
  public var hasAudio: Bool {return self._audio != nil}
  /// Clears the value of `audio`. Subsequent reads from it will return its default value.
  public mutating func clearAudio() {self._audio = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _config: Google_Cloud_Speech_V1_RecognitionConfig? = nil
  fileprivate var _audio: Google_Cloud_Speech_V1_RecognitionAudio? = nil
}

/// The top-level message sent by the client for the `LongRunningRecognize`
/// method.
public struct Google_Cloud_Speech_V1_LongRunningRecognizeRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. Provides information to the recognizer that specifies how to
  /// process the request.
  public var config: Google_Cloud_Speech_V1_RecognitionConfig {
    get {return _config ?? Google_Cloud_Speech_V1_RecognitionConfig()}
    set {_config = newValue}
  }
  /// Returns true if `config` has been explicitly set.
  public var hasConfig: Bool {return self._config != nil}
  /// Clears the value of `config`. Subsequent reads from it will return its default value.
  public mutating func clearConfig() {self._config = nil}

  /// Required. The audio data to be recognized.
  public var audio: Google_Cloud_Speech_V1_RecognitionAudio {
    get {return _audio ?? Google_Cloud_Speech_V1_RecognitionAudio()}
    set {_audio = newValue}
  }
  /// Returns true if `audio` has been explicitly set.
  public var hasAudio: Bool {return self._audio != nil}
  /// Clears the value of `audio`. Subsequent reads from it will return its default value.
  public mutating func clearAudio() {self._audio = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _config: Google_Cloud_Speech_V1_RecognitionConfig? = nil
  fileprivate var _audio: Google_Cloud_Speech_V1_RecognitionAudio? = nil
}

/// The top-level message sent by the client for the `StreamingRecognize` method.
/// Multiple `StreamingRecognizeRequest` messages are sent. The first message
/// must contain a `streaming_config` message and must not contain
/// `audio_content`. All subsequent messages must contain `audio_content` and
/// must not contain a `streaming_config` message.
public struct Google_Cloud_Speech_V1_StreamingRecognizeRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The streaming request, which is either a streaming config or audio content.
  public var streamingRequest: Google_Cloud_Speech_V1_StreamingRecognizeRequest.OneOf_StreamingRequest? = nil

  /// Provides information to the recognizer that specifies how to process the
  /// request. The first `StreamingRecognizeRequest` message must contain a
  /// `streaming_config`  message.
  public var streamingConfig: Google_Cloud_Speech_V1_StreamingRecognitionConfig {
    get {
      if case .streamingConfig(let v)? = streamingRequest {return v}
      return Google_Cloud_Speech_V1_StreamingRecognitionConfig()
    }
    set {streamingRequest = .streamingConfig(newValue)}
  }

  /// The audio data to be recognized. Sequential chunks of audio data are sent
  /// in sequential `StreamingRecognizeRequest` messages. The first
  /// `StreamingRecognizeRequest` message must not contain `audio_content` data
  /// and all subsequent `StreamingRecognizeRequest` messages must contain
  /// `audio_content` data. The audio bytes must be encoded as specified in
  /// `RecognitionConfig`. Note: as with all bytes fields, proto buffers use a
  /// pure binary representation (not base64). See
  /// [content limits](https://cloud.google.com/speech-to-text/quotas#content).
  public var audioContent: Data {
    get {
      if case .audioContent(let v)? = streamingRequest {return v}
      return SwiftProtobuf.Internal.emptyData
    }
    set {streamingRequest = .audioContent(newValue)}
  }

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// The streaming request, which is either a streaming config or audio content.
  public enum OneOf_StreamingRequest: Equatable {
    /// Provides information to the recognizer that specifies how to process the
    /// request. The first `StreamingRecognizeRequest` message must contain a
    /// `streaming_config`  message.
    case streamingConfig(Google_Cloud_Speech_V1_StreamingRecognitionConfig)
    /// The audio data to be recognized. Sequential chunks of audio data are sent
    /// in sequential `StreamingRecognizeRequest` messages. The first
    /// `StreamingRecognizeRequest` message must not contain `audio_content` data
    /// and all subsequent `StreamingRecognizeRequest` messages must contain
    /// `audio_content` data. The audio bytes must be encoded as specified in
    /// `RecognitionConfig`. Note: as with all bytes fields, proto buffers use a
    /// pure binary representation (not base64). See
    /// [content limits](https://cloud.google.com/speech-to-text/quotas#content).
    case audioContent(Data)

  #if !swift(>=4.1)
    public static func ==(lhs: Google_Cloud_Speech_V1_StreamingRecognizeRequest.OneOf_StreamingRequest, rhs: Google_Cloud_Speech_V1_StreamingRecognizeRequest.OneOf_StreamingRequest) -> Bool {
      switch (lhs, rhs) {
      case (.streamingConfig(let l), .streamingConfig(let r)): return l == r
      case (.audioContent(let l), .audioContent(let r)): return l == r
      default: return false
      }
    }
  #endif
  }

  public init() {}
}

/// Provides information to the recognizer that specifies how to process the
/// request.
public struct Google_Cloud_Speech_V1_StreamingRecognitionConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. Provides information to the recognizer that specifies how to
  /// process the request.
  public var config: Google_Cloud_Speech_V1_RecognitionConfig {
    get {return _config ?? Google_Cloud_Speech_V1_RecognitionConfig()}
    set {_config = newValue}
  }
  /// Returns true if `config` has been explicitly set.
  public var hasConfig: Bool {return self._config != nil}
  /// Clears the value of `config`. Subsequent reads from it will return its default value.
  public mutating func clearConfig() {self._config = nil}

  /// If `false` or omitted, the recognizer will perform continuous
  /// recognition (continuing to wait for and process audio even if the user
  /// pauses speaking) until the client closes the input stream (gRPC API) or
  /// until the maximum time limit has been reached. May return multiple
  /// `StreamingRecognitionResult`s with the `is_final` flag set to `true`.
  ///
  /// If `true`, the recognizer will detect a single spoken utterance. When it
  /// detects that the user has paused or stopped speaking, it will return an
  /// `END_OF_SINGLE_UTTERANCE` event and cease recognition. It will return no
  /// more than one `StreamingRecognitionResult` with the `is_final` flag set to
  /// `true`.
  public var singleUtterance: Bool = false

  /// If `true`, interim results (tentative hypotheses) may be
  /// returned as they become available (these interim results are indicated with
  /// the `is_final=false` flag).
  /// If `false` or omitted, only `is_final=true` result(s) are returned.
  public var interimResults: Bool = false

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _config: Google_Cloud_Speech_V1_RecognitionConfig? = nil
}

/// Provides information to the recognizer that specifies how to process the
/// request.
public struct Google_Cloud_Speech_V1_RecognitionConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Encoding of audio data sent in all `RecognitionAudio` messages.
  /// This field is optional for `FLAC` and `WAV` audio files and required
  /// for all other audio formats. For details, see [AudioEncoding][google.cloud.speech.v1.RecognitionConfig.AudioEncoding].
  public var encoding: Google_Cloud_Speech_V1_RecognitionConfig.AudioEncoding = .encodingUnspecified

  /// Sample rate in Hertz of the audio data sent in all
  /// `RecognitionAudio` messages. Valid values are: 8000-48000.
  /// 16000 is optimal. For best results, set the sampling rate of the audio
  /// source to 16000 Hz. If that's not possible, use the native sample rate of
  /// the audio source (instead of re-sampling).
  /// This field is optional for FLAC and WAV audio files, but is
  /// required for all other audio formats. For details, see [AudioEncoding][google.cloud.speech.v1.RecognitionConfig.AudioEncoding].
  public var sampleRateHertz: Int32 = 0

  /// The number of channels in the input audio data.
  /// ONLY set this for MULTI-CHANNEL recognition.
  /// Valid values for LINEAR16 and FLAC are `1`-`8`.
  /// Valid values for OGG_OPUS are '1'-'254'.
  /// Valid value for MULAW, AMR, AMR_WB and SPEEX_WITH_HEADER_BYTE is only `1`.
  /// If `0` or omitted, defaults to one channel (mono).
  /// Note: We only recognize the first channel by default.
  /// To perform independent recognition on each channel set
  /// `enable_separate_recognition_per_channel` to 'true'.
  public var audioChannelCount: Int32 = 0

  /// This needs to be set to `true` explicitly and `audio_channel_count` > 1
  /// to get each channel recognized separately. The recognition result will
  /// contain a `channel_tag` field to state which channel that result belongs
  /// to. If this is not true, we will only recognize the first channel. The
  /// request is billed cumulatively for all channels recognized:
  /// `audio_channel_count` multiplied by the length of the audio.
  public var enableSeparateRecognitionPerChannel: Bool = false

  /// Required. The language of the supplied audio as a
  /// [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
  /// Example: "en-US".
  /// See [Language
  /// Support](https://cloud.google.com/speech-to-text/docs/languages) for a list
  /// of the currently supported language codes.
  public var languageCode: String = String()

  /// Maximum number of recognition hypotheses to be returned.
  /// Specifically, the maximum number of `SpeechRecognitionAlternative` messages
  /// within each `SpeechRecognitionResult`.
  /// The server may return fewer than `max_alternatives`.
  /// Valid values are `0`-`30`. A value of `0` or `1` will return a maximum of
  /// one. If omitted, will return a maximum of one.
  public var maxAlternatives: Int32 = 0

  /// If set to `true`, the server will attempt to filter out
  /// profanities, replacing all but the initial character in each filtered word
  /// with asterisks, e.g. "f***". If set to `false` or omitted, profanities
  /// won't be filtered out.
  public var profanityFilter: Bool = false

  /// Array of [SpeechContext][google.cloud.speech.v1.SpeechContext].
  /// A means to provide context to assist the speech recognition. For more
  /// information, see
  /// [speech
  /// adaptation](https://cloud.google.com/speech-to-text/docs/context-strength).
  public var speechContexts: [Google_Cloud_Speech_V1_SpeechContext] = []

  /// If `true`, the top result includes a list of words and
  /// the start and end time offsets (timestamps) for those words. If
  /// `false`, no word-level time offset information is returned. The default is
  /// `false`.
  public var enableWordTimeOffsets: Bool = false

  /// If 'true', adds punctuation to recognition result hypotheses.
  /// This feature is only available in select languages. Setting this for
  /// requests in other languages has no effect at all.
  /// The default 'false' value does not add punctuation to result hypotheses.
  /// Note: This is currently offered as an experimental service, complimentary
  /// to all users. In the future this may be exclusively available as a
  /// premium feature.
  public var enableAutomaticPunctuation: Bool = false

  /// Config to enable speaker diarization and set additional
  /// parameters to make diarization better suited for your application.
  /// Note: When this is enabled, we send all the words from the beginning of the
  /// audio for the top alternative in every consecutive STREAMING responses.
  /// This is done in order to improve our speaker tags as our models learn to
  /// identify the speakers in the conversation over time.
  /// For non-streaming requests, the diarization results will be provided only
  /// in the top alternative of the FINAL SpeechRecognitionResult.
  public var diarizationConfig: Google_Cloud_Speech_V1_SpeakerDiarizationConfig {
    get {return _diarizationConfig ?? Google_Cloud_Speech_V1_SpeakerDiarizationConfig()}
    set {_diarizationConfig = newValue}
  }
  /// Returns true if `diarizationConfig` has been explicitly set.
  public var hasDiarizationConfig: Bool {return self._diarizationConfig != nil}
  /// Clears the value of `diarizationConfig`. Subsequent reads from it will return its default value.
  public mutating func clearDiarizationConfig() {self._diarizationConfig = nil}

  /// Metadata regarding this request.
  public var metadata: Google_Cloud_Speech_V1_RecognitionMetadata {
    get {return _metadata ?? Google_Cloud_Speech_V1_RecognitionMetadata()}
    set {_metadata = newValue}
  }
  /// Returns true if `metadata` has been explicitly set.
  public var hasMetadata: Bool {return self._metadata != nil}
  /// Clears the value of `metadata`. Subsequent reads from it will return its default value.
  public mutating func clearMetadata() {self._metadata = nil}

  /// Which model to select for the given request. Select the model
  /// best suited to your domain to get best results. If a model is not
  /// explicitly specified, then we auto-select a model based on the parameters
  /// in the RecognitionConfig.
  /// <table>
  ///   <tr>
  ///     <td><b>Model</b></td>
  ///     <td><b>Description</b></td>
  ///   </tr>
  ///   <tr>
  ///     <td><code>command_and_search</code></td>
  ///     <td>Best for short queries such as voice commands or voice search.</td>
  ///   </tr>
  ///   <tr>
  ///     <td><code>phone_call</code></td>
  ///     <td>Best for audio that originated from a phone call (typically
  ///     recorded at an 8khz sampling rate).</td>
  ///   </tr>
  ///   <tr>
  ///     <td><code>video</code></td>
  ///     <td>Best for audio that originated from from video or includes multiple
  ///         speakers. Ideally the audio is recorded at a 16khz or greater
  ///         sampling rate. This is a premium model that costs more than the
  ///         standard rate.</td>
  ///   </tr>
  ///   <tr>
  ///     <td><code>default</code></td>
  ///     <td>Best for audio that is not one of the specific audio models.
  ///         For example, long-form audio. Ideally the audio is high-fidelity,
  ///         recorded at a 16khz or greater sampling rate.</td>
  ///   </tr>
  /// </table>
  public var model: String = String()

  /// Set to true to use an enhanced model for speech recognition.
  /// If `use_enhanced` is set to true and the `model` field is not set, then
  /// an appropriate enhanced model is chosen if an enhanced model exists for
  /// the audio.
  ///
  /// If `use_enhanced` is true and an enhanced version of the specified model
  /// does not exist, then the speech is recognized using the standard version
  /// of the specified model.
  public var useEnhanced: Bool = false

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// The encoding of the audio data sent in the request.
  ///
  /// All encodings support only 1 channel (mono) audio, unless the
  /// `audio_channel_count` and `enable_separate_recognition_per_channel` fields
  /// are set.
  ///
  /// For best results, the audio source should be captured and transmitted using
  /// a lossless encoding (`FLAC` or `LINEAR16`). The accuracy of the speech
  /// recognition can be reduced if lossy codecs are used to capture or transmit
  /// audio, particularly if background noise is present. Lossy codecs include
  /// `MULAW`, `AMR`, `AMR_WB`, `OGG_OPUS`, `SPEEX_WITH_HEADER_BYTE`, and `MP3`.
  ///
  /// The `FLAC` and `WAV` audio file formats include a header that describes the
  /// included audio content. You can request recognition for `WAV` files that
  /// contain either `LINEAR16` or `MULAW` encoded audio.
  /// If you send `FLAC` or `WAV` audio file format in
  /// your request, you do not need to specify an `AudioEncoding`; the audio
  /// encoding format is determined from the file header. If you specify
  /// an `AudioEncoding` when you send  send `FLAC` or `WAV` audio, the
  /// encoding configuration must match the encoding described in the audio
  /// header; otherwise the request returns an
  /// [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT] error code.
  public enum AudioEncoding: SwiftProtobuf.Enum {
    public typealias RawValue = Int

    /// Not specified.
    case encodingUnspecified // = 0

    /// Uncompressed 16-bit signed little-endian samples (Linear PCM).
    case linear16 // = 1

    /// `FLAC` (Free Lossless Audio
    /// Codec) is the recommended encoding because it is
    /// lossless--therefore recognition is not compromised--and
    /// requires only about half the bandwidth of `LINEAR16`. `FLAC` stream
    /// encoding supports 16-bit and 24-bit samples, however, not all fields in
    /// `STREAMINFO` are supported.
    case flac // = 2

    /// 8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law.
    case mulaw // = 3

    /// Adaptive Multi-Rate Narrowband codec. `sample_rate_hertz` must be 8000.
    case amr // = 4

    /// Adaptive Multi-Rate Wideband codec. `sample_rate_hertz` must be 16000.
    case amrWb // = 5

    /// Opus encoded audio frames in Ogg container
    /// ([OggOpus](https://wiki.xiph.org/OggOpus)).
    /// `sample_rate_hertz` must be one of 8000, 12000, 16000, 24000, or 48000.
    case oggOpus // = 6

    /// Although the use of lossy encodings is not recommended, if a very low
    /// bitrate encoding is required, `OGG_OPUS` is highly preferred over
    /// Speex encoding. The [Speex](https://speex.org/)  encoding supported by
    /// Cloud Speech API has a header byte in each block, as in MIME type
    /// `audio/x-speex-with-header-byte`.
    /// It is a variant of the RTP Speex encoding defined in
    /// [RFC 5574](https://tools.ietf.org/html/rfc5574).
    /// The stream is a sequence of blocks, one block per RTP packet. Each block
    /// starts with a byte containing the length of the block, in bytes, followed
    /// by one or more frames of Speex data, padded to an integral number of
    /// bytes (octets) as specified in RFC 5574. In other words, each RTP header
    /// is replaced with a single byte containing the block length. Only Speex
    /// wideband is supported. `sample_rate_hertz` must be 16000.
    case speexWithHeaderByte // = 7
    case UNRECOGNIZED(Int)

    public init() {
      self = .encodingUnspecified
    }

    public init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .encodingUnspecified
      case 1: self = .linear16
      case 2: self = .flac
      case 3: self = .mulaw
      case 4: self = .amr
      case 5: self = .amrWb
      case 6: self = .oggOpus
      case 7: self = .speexWithHeaderByte
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    public var rawValue: Int {
      switch self {
      case .encodingUnspecified: return 0
      case .linear16: return 1
      case .flac: return 2
      case .mulaw: return 3
      case .amr: return 4
      case .amrWb: return 5
      case .oggOpus: return 6
      case .speexWithHeaderByte: return 7
      case .UNRECOGNIZED(let i): return i
      }
    }

  }

  public init() {}

  fileprivate var _diarizationConfig: Google_Cloud_Speech_V1_SpeakerDiarizationConfig? = nil
  fileprivate var _metadata: Google_Cloud_Speech_V1_RecognitionMetadata? = nil
}

#if swift(>=4.2)

extension Google_Cloud_Speech_V1_RecognitionConfig.AudioEncoding: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Speech_V1_RecognitionConfig.AudioEncoding] = [
    .encodingUnspecified,
    .linear16,
    .flac,
    .mulaw,
    .amr,
    .amrWb,
    .oggOpus,
    .speexWithHeaderByte,
  ]
}

#endif  // swift(>=4.2)

/// Config to enable speaker diarization.
public struct Google_Cloud_Speech_V1_SpeakerDiarizationConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// If 'true', enables speaker detection for each recognized word in
  /// the top alternative of the recognition result using a speaker_tag provided
  /// in the WordInfo.
  public var enableSpeakerDiarization: Bool = false

  /// Minimum number of speakers in the conversation. This range gives you more
  /// flexibility by allowing the system to automatically determine the correct
  /// number of speakers. If not set, the default value is 2.
  public var minSpeakerCount: Int32 = 0

  /// Maximum number of speakers in the conversation. This range gives you more
  /// flexibility by allowing the system to automatically determine the correct
  /// number of speakers. If not set, the default value is 6.
  public var maxSpeakerCount: Int32 = 0

  /// Unused.
  public var speakerTag: Int32 = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Description of audio data to be recognized.
public struct Google_Cloud_Speech_V1_RecognitionMetadata {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The use case most closely describing the audio content to be recognized.
  public var interactionType: Google_Cloud_Speech_V1_RecognitionMetadata.InteractionType = .unspecified

  /// The industry vertical to which this speech recognition request most
  /// closely applies. This is most indicative of the topics contained
  /// in the audio.  Use the 6-digit NAICS code to identify the industry
  /// vertical - see https://www.naics.com/search/.
  public var industryNaicsCodeOfAudio: UInt32 = 0

  /// The audio type that most closely describes the audio being recognized.
  public var microphoneDistance: Google_Cloud_Speech_V1_RecognitionMetadata.MicrophoneDistance = .unspecified

  /// The original media the speech was recorded on.
  public var originalMediaType: Google_Cloud_Speech_V1_RecognitionMetadata.OriginalMediaType = .unspecified

  /// The type of device the speech was recorded with.
  public var recordingDeviceType: Google_Cloud_Speech_V1_RecognitionMetadata.RecordingDeviceType = .unspecified

  /// The device used to make the recording.  Examples 'Nexus 5X' or
  /// 'Polycom SoundStation IP 6000' or 'POTS' or 'VoIP' or
  /// 'Cardioid Microphone'.
  public var recordingDeviceName: String = String()

  /// Mime type of the original audio file.  For example `audio/m4a`,
  /// `audio/x-alaw-basic`, `audio/mp3`, `audio/3gpp`.
  /// A list of possible audio mime types is maintained at
  /// http://www.iana.org/assignments/media-types/media-types.xhtml#audio
  public var originalMimeType: String = String()

  /// Description of the content. Eg. "Recordings of federal supreme court
  /// hearings from 2012".
  public var audioTopic: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Use case categories that the audio recognition request can be described
  /// by.
  public enum InteractionType: SwiftProtobuf.Enum {
    public typealias RawValue = Int

    /// Use case is either unknown or is something other than one of the other
    /// values below.
    case unspecified // = 0

    /// Multiple people in a conversation or discussion. For example in a
    /// meeting with two or more people actively participating. Typically
    /// all the primary people speaking would be in the same room (if not,
    /// see PHONE_CALL)
    case discussion // = 1

    /// One or more persons lecturing or presenting to others, mostly
    /// uninterrupted.
    case presentation // = 2

    /// A phone-call or video-conference in which two or more people, who are
    /// not in the same room, are actively participating.
    case phoneCall // = 3

    /// A recorded message intended for another person to listen to.
    case voicemail // = 4

    /// Professionally produced audio (eg. TV Show, Podcast).
    case professionallyProduced // = 5

    /// Transcribe spoken questions and queries into text.
    case voiceSearch // = 6

    /// Transcribe voice commands, such as for controlling a device.
    case voiceCommand // = 7

    /// Transcribe speech to text to create a written document, such as a
    /// text-message, email or report.
    case dictation // = 8
    case UNRECOGNIZED(Int)

    public init() {
      self = .unspecified
    }

    public init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unspecified
      case 1: self = .discussion
      case 2: self = .presentation
      case 3: self = .phoneCall
      case 4: self = .voicemail
      case 5: self = .professionallyProduced
      case 6: self = .voiceSearch
      case 7: self = .voiceCommand
      case 8: self = .dictation
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    public var rawValue: Int {
      switch self {
      case .unspecified: return 0
      case .discussion: return 1
      case .presentation: return 2
      case .phoneCall: return 3
      case .voicemail: return 4
      case .professionallyProduced: return 5
      case .voiceSearch: return 6
      case .voiceCommand: return 7
      case .dictation: return 8
      case .UNRECOGNIZED(let i): return i
      }
    }

  }

  /// Enumerates the types of capture settings describing an audio file.
  public enum MicrophoneDistance: SwiftProtobuf.Enum {
    public typealias RawValue = Int

    /// Audio type is not known.
    case unspecified // = 0

    /// The audio was captured from a closely placed microphone. Eg. phone,
    /// dictaphone, or handheld microphone. Generally if there speaker is within
    /// 1 meter of the microphone.
    case nearfield // = 1

    /// The speaker if within 3 meters of the microphone.
    case midfield // = 2

    /// The speaker is more than 3 meters away from the microphone.
    case farfield // = 3
    case UNRECOGNIZED(Int)

    public init() {
      self = .unspecified
    }

    public init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unspecified
      case 1: self = .nearfield
      case 2: self = .midfield
      case 3: self = .farfield
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    public var rawValue: Int {
      switch self {
      case .unspecified: return 0
      case .nearfield: return 1
      case .midfield: return 2
      case .farfield: return 3
      case .UNRECOGNIZED(let i): return i
      }
    }

  }

  /// The original media the speech was recorded on.
  public enum OriginalMediaType: SwiftProtobuf.Enum {
    public typealias RawValue = Int

    /// Unknown original media type.
    case unspecified // = 0

    /// The speech data is an audio recording.
    case audio // = 1

    /// The speech data originally recorded on a video.
    case video // = 2
    case UNRECOGNIZED(Int)

    public init() {
      self = .unspecified
    }

    public init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unspecified
      case 1: self = .audio
      case 2: self = .video
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    public var rawValue: Int {
      switch self {
      case .unspecified: return 0
      case .audio: return 1
      case .video: return 2
      case .UNRECOGNIZED(let i): return i
      }
    }

  }

  /// The type of device the speech was recorded with.
  public enum RecordingDeviceType: SwiftProtobuf.Enum {
    public typealias RawValue = Int

    /// The recording device is unknown.
    case unspecified // = 0

    /// Speech was recorded on a smartphone.
    case smartphone // = 1

    /// Speech was recorded using a personal computer or tablet.
    case pc // = 2

    /// Speech was recorded over a phone line.
    case phoneLine // = 3

    /// Speech was recorded in a vehicle.
    case vehicle // = 4

    /// Speech was recorded outdoors.
    case otherOutdoorDevice // = 5

    /// Speech was recorded indoors.
    case otherIndoorDevice // = 6
    case UNRECOGNIZED(Int)

    public init() {
      self = .unspecified
    }

    public init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unspecified
      case 1: self = .smartphone
      case 2: self = .pc
      case 3: self = .phoneLine
      case 4: self = .vehicle
      case 5: self = .otherOutdoorDevice
      case 6: self = .otherIndoorDevice
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    public var rawValue: Int {
      switch self {
      case .unspecified: return 0
      case .smartphone: return 1
      case .pc: return 2
      case .phoneLine: return 3
      case .vehicle: return 4
      case .otherOutdoorDevice: return 5
      case .otherIndoorDevice: return 6
      case .UNRECOGNIZED(let i): return i
      }
    }

  }

  public init() {}
}

#if swift(>=4.2)

extension Google_Cloud_Speech_V1_RecognitionMetadata.InteractionType: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Speech_V1_RecognitionMetadata.InteractionType] = [
    .unspecified,
    .discussion,
    .presentation,
    .phoneCall,
    .voicemail,
    .professionallyProduced,
    .voiceSearch,
    .voiceCommand,
    .dictation,
  ]
}

extension Google_Cloud_Speech_V1_RecognitionMetadata.MicrophoneDistance: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Speech_V1_RecognitionMetadata.MicrophoneDistance] = [
    .unspecified,
    .nearfield,
    .midfield,
    .farfield,
  ]
}

extension Google_Cloud_Speech_V1_RecognitionMetadata.OriginalMediaType: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Speech_V1_RecognitionMetadata.OriginalMediaType] = [
    .unspecified,
    .audio,
    .video,
  ]
}

extension Google_Cloud_Speech_V1_RecognitionMetadata.RecordingDeviceType: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Speech_V1_RecognitionMetadata.RecordingDeviceType] = [
    .unspecified,
    .smartphone,
    .pc,
    .phoneLine,
    .vehicle,
    .otherOutdoorDevice,
    .otherIndoorDevice,
  ]
}

#endif  // swift(>=4.2)

/// Provides "hints" to the speech recognizer to favor specific words and phrases
/// in the results.
public struct Google_Cloud_Speech_V1_SpeechContext {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// A list of strings containing words and phrases "hints" so that
  /// the speech recognition is more likely to recognize them. This can be used
  /// to improve the accuracy for specific words and phrases, for example, if
  /// specific commands are typically spoken by the user. This can also be used
  /// to add additional words to the vocabulary of the recognizer. See
  /// [usage limits](https://cloud.google.com/speech-to-text/quotas#content).
  ///
  /// List items can also be set to classes for groups of words that represent
  /// common concepts that occur in natural language. For example, rather than
  /// providing phrase hints for every month of the year, using the $MONTH class
  /// improves the likelihood of correctly transcribing audio that includes
  /// months.
  public var phrases: [String] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Contains audio data in the encoding specified in the `RecognitionConfig`.
/// Either `content` or `uri` must be supplied. Supplying both or neither
/// returns [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]. See
/// [content limits](https://cloud.google.com/speech-to-text/quotas#content).
public struct Google_Cloud_Speech_V1_RecognitionAudio {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The audio source, which is either inline content or a Google Cloud
  /// Storage uri.
  public var audioSource: Google_Cloud_Speech_V1_RecognitionAudio.OneOf_AudioSource? = nil

  /// The audio data bytes encoded as specified in
  /// `RecognitionConfig`. Note: as with all bytes fields, proto buffers use a
  /// pure binary representation, whereas JSON representations use base64.
  public var content: Data {
    get {
      if case .content(let v)? = audioSource {return v}
      return SwiftProtobuf.Internal.emptyData
    }
    set {audioSource = .content(newValue)}
  }

  /// URI that points to a file that contains audio data bytes as specified in
  /// `RecognitionConfig`. The file must not be compressed (for example, gzip).
  /// Currently, only Google Cloud Storage URIs are
  /// supported, which must be specified in the following format:
  /// `gs://bucket_name/object_name` (other URI formats return
  /// [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For more information, see
  /// [Request URIs](https://cloud.google.com/storage/docs/reference-uris).
  public var uri: String {
    get {
      if case .uri(let v)? = audioSource {return v}
      return String()
    }
    set {audioSource = .uri(newValue)}
  }

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// The audio source, which is either inline content or a Google Cloud
  /// Storage uri.
  public enum OneOf_AudioSource: Equatable {
    /// The audio data bytes encoded as specified in
    /// `RecognitionConfig`. Note: as with all bytes fields, proto buffers use a
    /// pure binary representation, whereas JSON representations use base64.
    case content(Data)
    /// URI that points to a file that contains audio data bytes as specified in
    /// `RecognitionConfig`. The file must not be compressed (for example, gzip).
    /// Currently, only Google Cloud Storage URIs are
    /// supported, which must be specified in the following format:
    /// `gs://bucket_name/object_name` (other URI formats return
    /// [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For more information, see
    /// [Request URIs](https://cloud.google.com/storage/docs/reference-uris).
    case uri(String)

  #if !swift(>=4.1)
    public static func ==(lhs: Google_Cloud_Speech_V1_RecognitionAudio.OneOf_AudioSource, rhs: Google_Cloud_Speech_V1_RecognitionAudio.OneOf_AudioSource) -> Bool {
      switch (lhs, rhs) {
      case (.content(let l), .content(let r)): return l == r
      case (.uri(let l), .uri(let r)): return l == r
      default: return false
      }
    }
  #endif
  }

  public init() {}
}

/// The only message returned to the client by the `Recognize` method. It
/// contains the result as zero or more sequential `SpeechRecognitionResult`
/// messages.
public struct Google_Cloud_Speech_V1_RecognizeResponse {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Sequential list of transcription results corresponding to
  /// sequential portions of audio.
  public var results: [Google_Cloud_Speech_V1_SpeechRecognitionResult] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// The only message returned to the client by the `LongRunningRecognize` method.
/// It contains the result as zero or more sequential `SpeechRecognitionResult`
/// messages. It is included in the `result.response` field of the `Operation`
/// returned by the `GetOperation` call of the `google::longrunning::Operations`
/// service.
public struct Google_Cloud_Speech_V1_LongRunningRecognizeResponse {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Sequential list of transcription results corresponding to
  /// sequential portions of audio.
  public var results: [Google_Cloud_Speech_V1_SpeechRecognitionResult] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Describes the progress of a long-running `LongRunningRecognize` call. It is
/// included in the `metadata` field of the `Operation` returned by the
/// `GetOperation` call of the `google::longrunning::Operations` service.
public struct Google_Cloud_Speech_V1_LongRunningRecognizeMetadata {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Approximate percentage of audio processed thus far. Guaranteed to be 100
  /// when the audio is fully processed and the results are available.
  public var progressPercent: Int32 = 0

  /// Time when the request was received.
  public var startTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _startTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_startTime = newValue}
  }
  /// Returns true if `startTime` has been explicitly set.
  public var hasStartTime: Bool {return self._startTime != nil}
  /// Clears the value of `startTime`. Subsequent reads from it will return its default value.
  public mutating func clearStartTime() {self._startTime = nil}

  /// Time of the most recent processing update.
  public var lastUpdateTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _lastUpdateTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_lastUpdateTime = newValue}
  }
  /// Returns true if `lastUpdateTime` has been explicitly set.
  public var hasLastUpdateTime: Bool {return self._lastUpdateTime != nil}
  /// Clears the value of `lastUpdateTime`. Subsequent reads from it will return its default value.
  public mutating func clearLastUpdateTime() {self._lastUpdateTime = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _startTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
  fileprivate var _lastUpdateTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
}

/// `StreamingRecognizeResponse` is the only message returned to the client by
/// `StreamingRecognize`. A series of zero or more `StreamingRecognizeResponse`
/// messages are streamed back to the client. If there is no recognizable
/// audio, and `single_utterance` is set to false, then no messages are streamed
/// back to the client.
///
/// Here's an example of a series of ten `StreamingRecognizeResponse`s that might
/// be returned while processing audio:
///
/// 1. results { alternatives { transcript: "tube" } stability: 0.01 }
///
/// 2. results { alternatives { transcript: "to be a" } stability: 0.01 }
///
/// 3. results { alternatives { transcript: "to be" } stability: 0.9 }
///    results { alternatives { transcript: " or not to be" } stability: 0.01 }
///
/// 4. results { alternatives { transcript: "to be or not to be"
///                             confidence: 0.92 }
///              alternatives { transcript: "to bee or not to bee" }
///              is_final: true }
///
/// 5. results { alternatives { transcript: " that's" } stability: 0.01 }
///
/// 6. results { alternatives { transcript: " that is" } stability: 0.9 }
///    results { alternatives { transcript: " the question" } stability: 0.01 }
///
/// 7. results { alternatives { transcript: " that is the question"
///                             confidence: 0.98 }
///              alternatives { transcript: " that was the question" }
///              is_final: true }
///
/// Notes:
///
/// - Only two of the above responses #4 and #7 contain final results; they are
///   indicated by `is_final: true`. Concatenating these together generates the
///   full transcript: "to be or not to be that is the question".
///
/// - The others contain interim `results`. #3 and #6 contain two interim
///   `results`: the first portion has a high stability and is less likely to
///   change; the second portion has a low stability and is very likely to
///   change. A UI designer might choose to show only high stability `results`.
///
/// - The specific `stability` and `confidence` values shown above are only for
///   illustrative purposes. Actual values may vary.
///
/// - In each response, only one of these fields will be set:
///     `error`,
///     `speech_event_type`, or
///     one or more (repeated) `results`.
public struct Google_Cloud_Speech_V1_StreamingRecognizeResponse {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// If set, returns a [google.rpc.Status][google.rpc.Status] message that
  /// specifies the error for the operation.
  public var error: Google_Rpc_Status {
    get {return _error ?? Google_Rpc_Status()}
    set {_error = newValue}
  }
  /// Returns true if `error` has been explicitly set.
  public var hasError: Bool {return self._error != nil}
  /// Clears the value of `error`. Subsequent reads from it will return its default value.
  public mutating func clearError() {self._error = nil}

  /// This repeated list contains zero or more results that
  /// correspond to consecutive portions of the audio currently being processed.
  /// It contains zero or one `is_final=true` result (the newly settled portion),
  /// followed by zero or more `is_final=false` results (the interim results).
  public var results: [Google_Cloud_Speech_V1_StreamingRecognitionResult] = []

  /// Indicates the type of speech event.
  public var speechEventType: Google_Cloud_Speech_V1_StreamingRecognizeResponse.SpeechEventType = .speechEventUnspecified

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Indicates the type of speech event.
  public enum SpeechEventType: SwiftProtobuf.Enum {
    public typealias RawValue = Int

    /// No speech event specified.
    case speechEventUnspecified // = 0

    /// This event indicates that the server has detected the end of the user's
    /// speech utterance and expects no additional speech. Therefore, the server
    /// will not process additional audio (although it may subsequently return
    /// additional results). The client should stop sending additional audio
    /// data, half-close the gRPC connection, and wait for any additional results
    /// until the server closes the gRPC connection. This event is only sent if
    /// `single_utterance` was set to `true`, and is not used otherwise.
    case endOfSingleUtterance // = 1
    case UNRECOGNIZED(Int)

    public init() {
      self = .speechEventUnspecified
    }

    public init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .speechEventUnspecified
      case 1: self = .endOfSingleUtterance
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    public var rawValue: Int {
      switch self {
      case .speechEventUnspecified: return 0
      case .endOfSingleUtterance: return 1
      case .UNRECOGNIZED(let i): return i
      }
    }

  }

  public init() {}

  fileprivate var _error: Google_Rpc_Status? = nil
}

#if swift(>=4.2)

extension Google_Cloud_Speech_V1_StreamingRecognizeResponse.SpeechEventType: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Speech_V1_StreamingRecognizeResponse.SpeechEventType] = [
    .speechEventUnspecified,
    .endOfSingleUtterance,
  ]
}

#endif  // swift(>=4.2)

/// A streaming speech recognition result corresponding to a portion of the audio
/// that is currently being processed.
public struct Google_Cloud_Speech_V1_StreamingRecognitionResult {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// May contain one or more recognition hypotheses (up to the
  /// maximum specified in `max_alternatives`).
  /// These alternatives are ordered in terms of accuracy, with the top (first)
  /// alternative being the most probable, as ranked by the recognizer.
  public var alternatives: [Google_Cloud_Speech_V1_SpeechRecognitionAlternative] = []

  /// If `false`, this `StreamingRecognitionResult` represents an
  /// interim result that may change. If `true`, this is the final time the
  /// speech service will return this particular `StreamingRecognitionResult`,
  /// the recognizer will not return any further hypotheses for this portion of
  /// the transcript and corresponding audio.
  public var isFinal: Bool = false

  /// An estimate of the likelihood that the recognizer will not
  /// change its guess about this interim result. Values range from 0.0
  /// (completely unstable) to 1.0 (completely stable).
  /// This field is only provided for interim results (`is_final=false`).
  /// The default of 0.0 is a sentinel value indicating `stability` was not set.
  public var stability: Float = 0

  /// Time offset of the end of this result relative to the
  /// beginning of the audio.
  public var resultEndTime: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _resultEndTime ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_resultEndTime = newValue}
  }
  /// Returns true if `resultEndTime` has been explicitly set.
  public var hasResultEndTime: Bool {return self._resultEndTime != nil}
  /// Clears the value of `resultEndTime`. Subsequent reads from it will return its default value.
  public mutating func clearResultEndTime() {self._resultEndTime = nil}

  /// For multi-channel audio, this is the channel number corresponding to the
  /// recognized result for the audio from that channel.
  /// For audio_channel_count = N, its output values can range from '1' to 'N'.
  public var channelTag: Int32 = 0

  /// The [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag of
  /// the language in this result. This language code was detected to have the
  /// most likelihood of being spoken in the audio.
  public var languageCode: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _resultEndTime: SwiftProtobuf.Google_Protobuf_Duration? = nil
}

/// A speech recognition result corresponding to a portion of the audio.
public struct Google_Cloud_Speech_V1_SpeechRecognitionResult {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// May contain one or more recognition hypotheses (up to the
  /// maximum specified in `max_alternatives`).
  /// These alternatives are ordered in terms of accuracy, with the top (first)
  /// alternative being the most probable, as ranked by the recognizer.
  public var alternatives: [Google_Cloud_Speech_V1_SpeechRecognitionAlternative] = []

  /// For multi-channel audio, this is the channel number corresponding to the
  /// recognized result for the audio from that channel.
  /// For audio_channel_count = N, its output values can range from '1' to 'N'.
  public var channelTag: Int32 = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Alternative hypotheses (a.k.a. n-best list).
public struct Google_Cloud_Speech_V1_SpeechRecognitionAlternative {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Transcript text representing the words that the user spoke.
  public var transcript: String = String()

  /// The confidence estimate between 0.0 and 1.0. A higher number
  /// indicates an estimated greater likelihood that the recognized words are
  /// correct. This field is set only for the top alternative of a non-streaming
  /// result or, of a streaming result where `is_final=true`.
  /// This field is not guaranteed to be accurate and users should not rely on it
  /// to be always provided.
  /// The default of 0.0 is a sentinel value indicating `confidence` was not set.
  public var confidence: Float = 0

  /// A list of word-specific information for each recognized word.
  /// Note: When `enable_speaker_diarization` is true, you will see all the words
  /// from the beginning of the audio.
  public var words: [Google_Cloud_Speech_V1_WordInfo] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Word-specific information for recognized words.
public struct Google_Cloud_Speech_V1_WordInfo {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Time offset relative to the beginning of the audio,
  /// and corresponding to the start of the spoken word.
  /// This field is only set if `enable_word_time_offsets=true` and only
  /// in the top hypothesis.
  /// This is an experimental feature and the accuracy of the time offset can
  /// vary.
  public var startTime: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _startTime ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_startTime = newValue}
  }
  /// Returns true if `startTime` has been explicitly set.
  public var hasStartTime: Bool {return self._startTime != nil}
  /// Clears the value of `startTime`. Subsequent reads from it will return its default value.
  public mutating func clearStartTime() {self._startTime = nil}

  /// Time offset relative to the beginning of the audio,
  /// and corresponding to the end of the spoken word.
  /// This field is only set if `enable_word_time_offsets=true` and only
  /// in the top hypothesis.
  /// This is an experimental feature and the accuracy of the time offset can
  /// vary.
  public var endTime: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _endTime ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_endTime = newValue}
  }
  /// Returns true if `endTime` has been explicitly set.
  public var hasEndTime: Bool {return self._endTime != nil}
  /// Clears the value of `endTime`. Subsequent reads from it will return its default value.
  public mutating func clearEndTime() {self._endTime = nil}

  /// The word corresponding to this set of information.
  public var word: String = String()

  /// A distinct integer value is assigned for every speaker within
  /// the audio. This field specifies which one of those speakers was detected to
  /// have spoken this word. Value ranges from '1' to diarization_speaker_count.
  /// speaker_tag is set if enable_speaker_diarization = 'true' and only in the
  /// top alternative.
  public var speakerTag: Int32 = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _startTime: SwiftProtobuf.Google_Protobuf_Duration? = nil
  fileprivate var _endTime: SwiftProtobuf.Google_Protobuf_Duration? = nil
}

// MARK: - Code below here is support for the SwiftProtobuf runtime.

fileprivate let _protobuf_package = "google.cloud.speech.v1"

extension Google_Cloud_Speech_V1_RecognizeRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".RecognizeRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "config"),
    2: .same(proto: "audio"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      switch fieldNumber {
      case 1: try decoder.decodeSingularMessageField(value: &self._config)
      case 2: try decoder.decodeSingularMessageField(value: &self._audio)
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._config {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if let v = self._audio {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1_RecognizeRequest, rhs: Google_Cloud_Speech_V1_RecognizeRequest) -> Bool {
    if lhs._config != rhs._config {return false}
    if lhs._audio != rhs._audio {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1_LongRunningRecognizeRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".LongRunningRecognizeRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "config"),
    2: .same(proto: "audio"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      switch fieldNumber {
      case 1: try decoder.decodeSingularMessageField(value: &self._config)
      case 2: try decoder.decodeSingularMessageField(value: &self._audio)
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._config {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if let v = self._audio {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1_LongRunningRecognizeRequest, rhs: Google_Cloud_Speech_V1_LongRunningRecognizeRequest) -> Bool {
    if lhs._config != rhs._config {return false}
    if lhs._audio != rhs._audio {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1_StreamingRecognizeRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".StreamingRecognizeRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "streaming_config"),
    2: .standard(proto: "audio_content"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      switch fieldNumber {
      case 1:
        var v: Google_Cloud_Speech_V1_StreamingRecognitionConfig?
        if let current = self.streamingRequest {
          try decoder.handleConflictingOneOf()
          if case .streamingConfig(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.streamingRequest = .streamingConfig(v)}
      case 2:
        if self.streamingRequest != nil {try decoder.handleConflictingOneOf()}
        var v: Data?
        try decoder.decodeSingularBytesField(value: &v)
        if let v = v {self.streamingRequest = .audioContent(v)}
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    switch self.streamingRequest {
    case .streamingConfig(let v)?:
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    case .audioContent(let v)?:
      try visitor.visitSingularBytesField(value: v, fieldNumber: 2)
    case nil: break
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1_StreamingRecognizeRequest, rhs: Google_Cloud_Speech_V1_StreamingRecognizeRequest) -> Bool {
    if lhs.streamingRequest != rhs.streamingRequest {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1_StreamingRecognitionConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".StreamingRecognitionConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "config"),
    2: .standard(proto: "single_utterance"),
    3: .standard(proto: "interim_results"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      switch fieldNumber {
      case 1: try decoder.decodeSingularMessageField(value: &self._config)
      case 2: try decoder.decodeSingularBoolField(value: &self.singleUtterance)
      case 3: try decoder.decodeSingularBoolField(value: &self.interimResults)
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._config {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if self.singleUtterance != false {
      try visitor.visitSingularBoolField(value: self.singleUtterance, fieldNumber: 2)
    }
    if self.interimResults != false {
      try visitor.visitSingularBoolField(value: self.interimResults, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1_StreamingRecognitionConfig, rhs: Google_Cloud_Speech_V1_StreamingRecognitionConfig) -> Bool {
    if lhs._config != rhs._config {return false}
    if lhs.singleUtterance != rhs.singleUtterance {return false}
    if lhs.interimResults != rhs.interimResults {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1_RecognitionConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".RecognitionConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "encoding"),
    2: .standard(proto: "sample_rate_hertz"),
    7: .standard(proto: "audio_channel_count"),
    12: .standard(proto: "enable_separate_recognition_per_channel"),
    3: .standard(proto: "language_code"),
    4: .standard(proto: "max_alternatives"),
    5: .standard(proto: "profanity_filter"),
    6: .standard(proto: "speech_contexts"),
    8: .standard(proto: "enable_word_time_offsets"),
    11: .standard(proto: "enable_automatic_punctuation"),
    19: .standard(proto: "diarization_config"),
    9: .same(proto: "metadata"),
    13: .same(proto: "model"),
    14: .standard(proto: "use_enhanced"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      switch fieldNumber {
      case 1: try decoder.decodeSingularEnumField(value: &self.encoding)
      case 2: try decoder.decodeSingularInt32Field(value: &self.sampleRateHertz)
      case 3: try decoder.decodeSingularStringField(value: &self.languageCode)
      case 4: try decoder.decodeSingularInt32Field(value: &self.maxAlternatives)
      case 5: try decoder.decodeSingularBoolField(value: &self.profanityFilter)
      case 6: try decoder.decodeRepeatedMessageField(value: &self.speechContexts)
      case 7: try decoder.decodeSingularInt32Field(value: &self.audioChannelCount)
      case 8: try decoder.decodeSingularBoolField(value: &self.enableWordTimeOffsets)
      case 9: try decoder.decodeSingularMessageField(value: &self._metadata)
      case 11: try decoder.decodeSingularBoolField(value: &self.enableAutomaticPunctuation)
      case 12: try decoder.decodeSingularBoolField(value: &self.enableSeparateRecognitionPerChannel)
      case 13: try decoder.decodeSingularStringField(value: &self.model)
      case 14: try decoder.decodeSingularBoolField(value: &self.useEnhanced)
      case 19: try decoder.decodeSingularMessageField(value: &self._diarizationConfig)
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.encoding != .encodingUnspecified {
      try visitor.visitSingularEnumField(value: self.encoding, fieldNumber: 1)
    }
    if self.sampleRateHertz != 0 {
      try visitor.visitSingularInt32Field(value: self.sampleRateHertz, fieldNumber: 2)
    }
    if !self.languageCode.isEmpty {
      try visitor.visitSingularStringField(value: self.languageCode, fieldNumber: 3)
    }
    if self.maxAlternatives != 0 {
      try visitor.visitSingularInt32Field(value: self.maxAlternatives, fieldNumber: 4)
    }
    if self.profanityFilter != false {
      try visitor.visitSingularBoolField(value: self.profanityFilter, fieldNumber: 5)
    }
    if !self.speechContexts.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.speechContexts, fieldNumber: 6)
    }
    if self.audioChannelCount != 0 {
      try visitor.visitSingularInt32Field(value: self.audioChannelCount, fieldNumber: 7)
    }
    if self.enableWordTimeOffsets != false {
      try visitor.visitSingularBoolField(value: self.enableWordTimeOffsets, fieldNumber: 8)
    }
    if let v = self._metadata {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 9)
    }
    if self.enableAutomaticPunctuation != false {
      try visitor.visitSingularBoolField(value: self.enableAutomaticPunctuation, fieldNumber: 11)
    }
    if self.enableSeparateRecognitionPerChannel != false {
      try visitor.visitSingularBoolField(value: self.enableSeparateRecognitionPerChannel, fieldNumber: 12)
    }
    if !self.model.isEmpty {
      try visitor.visitSingularStringField(value: self.model, fieldNumber: 13)
    }
    if self.useEnhanced != false {
      try visitor.visitSingularBoolField(value: self.useEnhanced, fieldNumber: 14)
    }
    if let v = self._diarizationConfig {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 19)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1_RecognitionConfig, rhs: Google_Cloud_Speech_V1_RecognitionConfig) -> Bool {
    if lhs.encoding != rhs.encoding {return false}
    if lhs.sampleRateHertz != rhs.sampleRateHertz {return false}
    if lhs.audioChannelCount != rhs.audioChannelCount {return false}
    if lhs.enableSeparateRecognitionPerChannel != rhs.enableSeparateRecognitionPerChannel {return false}
    if lhs.languageCode != rhs.languageCode {return false}
    if lhs.maxAlternatives != rhs.maxAlternatives {return false}
    if lhs.profanityFilter != rhs.profanityFilter {return false}
    if lhs.speechContexts != rhs.speechContexts {return false}
    if lhs.enableWordTimeOffsets != rhs.enableWordTimeOffsets {return false}
    if lhs.enableAutomaticPunctuation != rhs.enableAutomaticPunctuation {return false}
    if lhs._diarizationConfig != rhs._diarizationConfig {return false}
    if lhs._metadata != rhs._metadata {return false}
    if lhs.model != rhs.model {return false}
    if lhs.useEnhanced != rhs.useEnhanced {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1_RecognitionConfig.AudioEncoding: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "ENCODING_UNSPECIFIED"),
    1: .same(proto: "LINEAR16"),
    2: .same(proto: "FLAC"),
    3: .same(proto: "MULAW"),
    4: .same(proto: "AMR"),
    5: .same(proto: "AMR_WB"),
    6: .same(proto: "OGG_OPUS"),
    7: .same(proto: "SPEEX_WITH_HEADER_BYTE"),
  ]
}

extension Google_Cloud_Speech_V1_SpeakerDiarizationConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".SpeakerDiarizationConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "enable_speaker_diarization"),
    2: .standard(proto: "min_speaker_count"),
    3: .standard(proto: "max_speaker_count"),
    5: .standard(proto: "speaker_tag"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      switch fieldNumber {
      case 1: try decoder.decodeSingularBoolField(value: &self.enableSpeakerDiarization)
      case 2: try decoder.decodeSingularInt32Field(value: &self.minSpeakerCount)
      case 3: try decoder.decodeSingularInt32Field(value: &self.maxSpeakerCount)
      case 5: try decoder.decodeSingularInt32Field(value: &self.speakerTag)
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.enableSpeakerDiarization != false {
      try visitor.visitSingularBoolField(value: self.enableSpeakerDiarization, fieldNumber: 1)
    }
    if self.minSpeakerCount != 0 {
      try visitor.visitSingularInt32Field(value: self.minSpeakerCount, fieldNumber: 2)
    }
    if self.maxSpeakerCount != 0 {
      try visitor.visitSingularInt32Field(value: self.maxSpeakerCount, fieldNumber: 3)
    }
    if self.speakerTag != 0 {
      try visitor.visitSingularInt32Field(value: self.speakerTag, fieldNumber: 5)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1_SpeakerDiarizationConfig, rhs: Google_Cloud_Speech_V1_SpeakerDiarizationConfig) -> Bool {
    if lhs.enableSpeakerDiarization != rhs.enableSpeakerDiarization {return false}
    if lhs.minSpeakerCount != rhs.minSpeakerCount {return false}
    if lhs.maxSpeakerCount != rhs.maxSpeakerCount {return false}
    if lhs.speakerTag != rhs.speakerTag {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1_RecognitionMetadata: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".RecognitionMetadata"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "interaction_type"),
    3: .standard(proto: "industry_naics_code_of_audio"),
    4: .standard(proto: "microphone_distance"),
    5: .standard(proto: "original_media_type"),
    6: .standard(proto: "recording_device_type"),
    7: .standard(proto: "recording_device_name"),
    8: .standard(proto: "original_mime_type"),
    10: .standard(proto: "audio_topic"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      switch fieldNumber {
      case 1: try decoder.decodeSingularEnumField(value: &self.interactionType)
      case 3: try decoder.decodeSingularUInt32Field(value: &self.industryNaicsCodeOfAudio)
      case 4: try decoder.decodeSingularEnumField(value: &self.microphoneDistance)
      case 5: try decoder.decodeSingularEnumField(value: &self.originalMediaType)
      case 6: try decoder.decodeSingularEnumField(value: &self.recordingDeviceType)
      case 7: try decoder.decodeSingularStringField(value: &self.recordingDeviceName)
      case 8: try decoder.decodeSingularStringField(value: &self.originalMimeType)
      case 10: try decoder.decodeSingularStringField(value: &self.audioTopic)
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.interactionType != .unspecified {
      try visitor.visitSingularEnumField(value: self.interactionType, fieldNumber: 1)
    }
    if self.industryNaicsCodeOfAudio != 0 {
      try visitor.visitSingularUInt32Field(value: self.industryNaicsCodeOfAudio, fieldNumber: 3)
    }
    if self.microphoneDistance != .unspecified {
      try visitor.visitSingularEnumField(value: self.microphoneDistance, fieldNumber: 4)
    }
    if self.originalMediaType != .unspecified {
      try visitor.visitSingularEnumField(value: self.originalMediaType, fieldNumber: 5)
    }
    if self.recordingDeviceType != .unspecified {
      try visitor.visitSingularEnumField(value: self.recordingDeviceType, fieldNumber: 6)
    }
    if !self.recordingDeviceName.isEmpty {
      try visitor.visitSingularStringField(value: self.recordingDeviceName, fieldNumber: 7)
    }
    if !self.originalMimeType.isEmpty {
      try visitor.visitSingularStringField(value: self.originalMimeType, fieldNumber: 8)
    }
    if !self.audioTopic.isEmpty {
      try visitor.visitSingularStringField(value: self.audioTopic, fieldNumber: 10)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1_RecognitionMetadata, rhs: Google_Cloud_Speech_V1_RecognitionMetadata) -> Bool {
    if lhs.interactionType != rhs.interactionType {return false}
    if lhs.industryNaicsCodeOfAudio != rhs.industryNaicsCodeOfAudio {return false}
    if lhs.microphoneDistance != rhs.microphoneDistance {return false}
    if lhs.originalMediaType != rhs.originalMediaType {return false}
    if lhs.recordingDeviceType != rhs.recordingDeviceType {return false}
    if lhs.recordingDeviceName != rhs.recordingDeviceName {return false}
    if lhs.originalMimeType != rhs.originalMimeType {return false}
    if lhs.audioTopic != rhs.audioTopic {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1_RecognitionMetadata.InteractionType: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "INTERACTION_TYPE_UNSPECIFIED"),
    1: .same(proto: "DISCUSSION"),
    2: .same(proto: "PRESENTATION"),
    3: .same(proto: "PHONE_CALL"),
    4: .same(proto: "VOICEMAIL"),
    5: .same(proto: "PROFESSIONALLY_PRODUCED"),
    6: .same(proto: "VOICE_SEARCH"),
    7: .same(proto: "VOICE_COMMAND"),
    8: .same(proto: "DICTATION"),
  ]
}

extension Google_Cloud_Speech_V1_RecognitionMetadata.MicrophoneDistance: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "MICROPHONE_DISTANCE_UNSPECIFIED"),
    1: .same(proto: "NEARFIELD"),
    2: .same(proto: "MIDFIELD"),
    3: .same(proto: "FARFIELD"),
  ]
}

extension Google_Cloud_Speech_V1_RecognitionMetadata.OriginalMediaType: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "ORIGINAL_MEDIA_TYPE_UNSPECIFIED"),
    1: .same(proto: "AUDIO"),
    2: .same(proto: "VIDEO"),
  ]
}

extension Google_Cloud_Speech_V1_RecognitionMetadata.RecordingDeviceType: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "RECORDING_DEVICE_TYPE_UNSPECIFIED"),
    1: .same(proto: "SMARTPHONE"),
    2: .same(proto: "PC"),
    3: .same(proto: "PHONE_LINE"),
    4: .same(proto: "VEHICLE"),
    5: .same(proto: "OTHER_OUTDOOR_DEVICE"),
    6: .same(proto: "OTHER_INDOOR_DEVICE"),
  ]
}

extension Google_Cloud_Speech_V1_SpeechContext: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".SpeechContext"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "phrases"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      switch fieldNumber {
      case 1: try decoder.decodeRepeatedStringField(value: &self.phrases)
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.phrases.isEmpty {
      try visitor.visitRepeatedStringField(value: self.phrases, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1_SpeechContext, rhs: Google_Cloud_Speech_V1_SpeechContext) -> Bool {
    if lhs.phrases != rhs.phrases {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1_RecognitionAudio: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".RecognitionAudio"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "content"),
    2: .same(proto: "uri"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      switch fieldNumber {
      case 1:
        if self.audioSource != nil {try decoder.handleConflictingOneOf()}
        var v: Data?
        try decoder.decodeSingularBytesField(value: &v)
        if let v = v {self.audioSource = .content(v)}
      case 2:
        if self.audioSource != nil {try decoder.handleConflictingOneOf()}
        var v: String?
        try decoder.decodeSingularStringField(value: &v)
        if let v = v {self.audioSource = .uri(v)}
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    switch self.audioSource {
    case .content(let v)?:
      try visitor.visitSingularBytesField(value: v, fieldNumber: 1)
    case .uri(let v)?:
      try visitor.visitSingularStringField(value: v, fieldNumber: 2)
    case nil: break
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1_RecognitionAudio, rhs: Google_Cloud_Speech_V1_RecognitionAudio) -> Bool {
    if lhs.audioSource != rhs.audioSource {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1_RecognizeResponse: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".RecognizeResponse"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    2: .same(proto: "results"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      switch fieldNumber {
      case 2: try decoder.decodeRepeatedMessageField(value: &self.results)
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.results.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.results, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1_RecognizeResponse, rhs: Google_Cloud_Speech_V1_RecognizeResponse) -> Bool {
    if lhs.results != rhs.results {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1_LongRunningRecognizeResponse: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".LongRunningRecognizeResponse"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    2: .same(proto: "results"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      switch fieldNumber {
      case 2: try decoder.decodeRepeatedMessageField(value: &self.results)
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.results.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.results, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1_LongRunningRecognizeResponse, rhs: Google_Cloud_Speech_V1_LongRunningRecognizeResponse) -> Bool {
    if lhs.results != rhs.results {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1_LongRunningRecognizeMetadata: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".LongRunningRecognizeMetadata"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "progress_percent"),
    2: .standard(proto: "start_time"),
    3: .standard(proto: "last_update_time"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      switch fieldNumber {
      case 1: try decoder.decodeSingularInt32Field(value: &self.progressPercent)
      case 2: try decoder.decodeSingularMessageField(value: &self._startTime)
      case 3: try decoder.decodeSingularMessageField(value: &self._lastUpdateTime)
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.progressPercent != 0 {
      try visitor.visitSingularInt32Field(value: self.progressPercent, fieldNumber: 1)
    }
    if let v = self._startTime {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    if let v = self._lastUpdateTime {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1_LongRunningRecognizeMetadata, rhs: Google_Cloud_Speech_V1_LongRunningRecognizeMetadata) -> Bool {
    if lhs.progressPercent != rhs.progressPercent {return false}
    if lhs._startTime != rhs._startTime {return false}
    if lhs._lastUpdateTime != rhs._lastUpdateTime {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1_StreamingRecognizeResponse: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".StreamingRecognizeResponse"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "error"),
    2: .same(proto: "results"),
    4: .standard(proto: "speech_event_type"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      switch fieldNumber {
      case 1: try decoder.decodeSingularMessageField(value: &self._error)
      case 2: try decoder.decodeRepeatedMessageField(value: &self.results)
      case 4: try decoder.decodeSingularEnumField(value: &self.speechEventType)
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._error {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if !self.results.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.results, fieldNumber: 2)
    }
    if self.speechEventType != .speechEventUnspecified {
      try visitor.visitSingularEnumField(value: self.speechEventType, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1_StreamingRecognizeResponse, rhs: Google_Cloud_Speech_V1_StreamingRecognizeResponse) -> Bool {
    if lhs._error != rhs._error {return false}
    if lhs.results != rhs.results {return false}
    if lhs.speechEventType != rhs.speechEventType {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1_StreamingRecognizeResponse.SpeechEventType: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "SPEECH_EVENT_UNSPECIFIED"),
    1: .same(proto: "END_OF_SINGLE_UTTERANCE"),
  ]
}

extension Google_Cloud_Speech_V1_StreamingRecognitionResult: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".StreamingRecognitionResult"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "alternatives"),
    2: .standard(proto: "is_final"),
    3: .same(proto: "stability"),
    4: .standard(proto: "result_end_time"),
    5: .standard(proto: "channel_tag"),
    6: .standard(proto: "language_code"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      switch fieldNumber {
      case 1: try decoder.decodeRepeatedMessageField(value: &self.alternatives)
      case 2: try decoder.decodeSingularBoolField(value: &self.isFinal)
      case 3: try decoder.decodeSingularFloatField(value: &self.stability)
      case 4: try decoder.decodeSingularMessageField(value: &self._resultEndTime)
      case 5: try decoder.decodeSingularInt32Field(value: &self.channelTag)
      case 6: try decoder.decodeSingularStringField(value: &self.languageCode)
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.alternatives.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.alternatives, fieldNumber: 1)
    }
    if self.isFinal != false {
      try visitor.visitSingularBoolField(value: self.isFinal, fieldNumber: 2)
    }
    if self.stability != 0 {
      try visitor.visitSingularFloatField(value: self.stability, fieldNumber: 3)
    }
    if let v = self._resultEndTime {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
    }
    if self.channelTag != 0 {
      try visitor.visitSingularInt32Field(value: self.channelTag, fieldNumber: 5)
    }
    if !self.languageCode.isEmpty {
      try visitor.visitSingularStringField(value: self.languageCode, fieldNumber: 6)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1_StreamingRecognitionResult, rhs: Google_Cloud_Speech_V1_StreamingRecognitionResult) -> Bool {
    if lhs.alternatives != rhs.alternatives {return false}
    if lhs.isFinal != rhs.isFinal {return false}
    if lhs.stability != rhs.stability {return false}
    if lhs._resultEndTime != rhs._resultEndTime {return false}
    if lhs.channelTag != rhs.channelTag {return false}
    if lhs.languageCode != rhs.languageCode {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1_SpeechRecognitionResult: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".SpeechRecognitionResult"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "alternatives"),
    2: .standard(proto: "channel_tag"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      switch fieldNumber {
      case 1: try decoder.decodeRepeatedMessageField(value: &self.alternatives)
      case 2: try decoder.decodeSingularInt32Field(value: &self.channelTag)
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.alternatives.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.alternatives, fieldNumber: 1)
    }
    if self.channelTag != 0 {
      try visitor.visitSingularInt32Field(value: self.channelTag, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1_SpeechRecognitionResult, rhs: Google_Cloud_Speech_V1_SpeechRecognitionResult) -> Bool {
    if lhs.alternatives != rhs.alternatives {return false}
    if lhs.channelTag != rhs.channelTag {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1_SpeechRecognitionAlternative: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".SpeechRecognitionAlternative"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "transcript"),
    2: .same(proto: "confidence"),
    3: .same(proto: "words"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      switch fieldNumber {
      case 1: try decoder.decodeSingularStringField(value: &self.transcript)
      case 2: try decoder.decodeSingularFloatField(value: &self.confidence)
      case 3: try decoder.decodeRepeatedMessageField(value: &self.words)
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.transcript.isEmpty {
      try visitor.visitSingularStringField(value: self.transcript, fieldNumber: 1)
    }
    if self.confidence != 0 {
      try visitor.visitSingularFloatField(value: self.confidence, fieldNumber: 2)
    }
    if !self.words.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.words, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1_SpeechRecognitionAlternative, rhs: Google_Cloud_Speech_V1_SpeechRecognitionAlternative) -> Bool {
    if lhs.transcript != rhs.transcript {return false}
    if lhs.confidence != rhs.confidence {return false}
    if lhs.words != rhs.words {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1_WordInfo: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".WordInfo"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "start_time"),
    2: .standard(proto: "end_time"),
    3: .same(proto: "word"),
    5: .standard(proto: "speaker_tag"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      switch fieldNumber {
      case 1: try decoder.decodeSingularMessageField(value: &self._startTime)
      case 2: try decoder.decodeSingularMessageField(value: &self._endTime)
      case 3: try decoder.decodeSingularStringField(value: &self.word)
      case 5: try decoder.decodeSingularInt32Field(value: &self.speakerTag)
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._startTime {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if let v = self._endTime {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    if !self.word.isEmpty {
      try visitor.visitSingularStringField(value: self.word, fieldNumber: 3)
    }
    if self.speakerTag != 0 {
      try visitor.visitSingularInt32Field(value: self.speakerTag, fieldNumber: 5)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1_WordInfo, rhs: Google_Cloud_Speech_V1_WordInfo) -> Bool {
    if lhs._startTime != rhs._startTime {return false}
    if lhs._endTime != rhs._endTime {return false}
    if lhs.word != rhs.word {return false}
    if lhs.speakerTag != rhs.speakerTag {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}
